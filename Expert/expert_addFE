import cv2
import numpy as np
import sklearn
import glob
from skimage.feature import hog
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from skimage.feature import local_binary_pattern
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier


def SIFT(image):
    sift = cv2.SIFT_create()

    # 在图像上检测并提取关键点和描述符
    keypoints, descriptors = sift.detectAndCompute(image, None)

    # 可视化关键点
    image_with_keypoints = cv2.drawKeypoints(image, keypoints, None)
    # image_with_keypoints = cv2.cvtColor(image_with_keypoints, cv2.COLOR_BGR2RGB)

    return image_with_keypoints


def dataloader():
    dataset_path = "E:\\ProgramFiles\\codeProject\\python\\VC\\TrafficSignRecongnition-master\\dataset\\"
    X = []
    y = []
    for i in glob.glob(dataset_path + '*.png', recursive=True):
        label = i.split("dataset")[1][1:4]
        y.append(label)
        img = cv2.imread(i)

        X.append(img)

    return X, y


def processing_gray(X):
    X_processed = []
    for x in X:
        # Write code to resize image x to 48x48 and store in temp_x
        temp_x = cv2.resize(x, (48, 48))
        # Write code to convert temp_x to grayscale
        temp_x = cv2.cvtColor(temp_x, cv2.COLOR_BGR2GRAY)
        # Append the converted image into X_processed
        X_processed.append(temp_x)
    return X_processed


def processing_calcHist(X):
    X_processed = []
    for x in X:
        img = x
        (b, g, r) = cv2.split(img)
        bH = cv2.equalizeHist(b)
        gH = cv2.equalizeHist(g)
        rH = cv2.equalizeHist(r)
        result = cv2.merge((bH, gH, rH))
        result = cv2.resize(result, (48, 48))
        result = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)
        X_processed.append(result)
    return X_processed


def feature_extraction_HOG(X_processed):
    # Feature extraction
    X_features = []
    for x in X_processed:
        x_feature = hog(x, orientations=8, pixels_per_cell=(10, 10),
                        cells_per_block=(1, 1), visualize=False, multichannel=False)
        X_features.append(x_feature)
    return X_features


def feature_extraction_local_binary(X_processed):
    X_features = []
    for x in X_processed:
        radius = 3
        n_points = 8 * radius
        x_feature = local_binary_pattern(x, n_points, radius)
        x_feature = x_feature.reshape(-1)
        X_features.append(x_feature)
    return X_features


def feature_extraction_sift(X_processed):
    X_features = []
    descriptors_list = []
    countor = 0
    for x in X_processed:
        countor += 1
        sift = cv2.SIFT_create()
        keypoints, descriptors = sift.detectAndCompute(x, None)
        if (descriptors is None):
            descriptors = np.empty((0,))
        descriptors = descriptors.reshape(-1)
        descriptors_list.append(descriptors)
        image_with_keypoints = cv2.drawKeypoints(x, keypoints, None)
        x_feature = image_with_keypoints.reshape(-1)
        X_features.append(x_feature)
    max_length = max(len(member) for member in descriptors_list)
    descriptors_list = [np.pad(arr, (0, max_length - len(arr)), mode='constant') for arr in descriptors_list]

    X_features = [np.concatenate((x, y)) for x, y in zip(X_features, descriptors_list)]

    return X_features


def feature_extraction_Feature_Pyramid(X_processed):
    num_layers = 3
    X_features = []
    for x in X_processed:
        image = x
        gaussian_pyr = [image]
        for i in range(1, num_layers):
            image = cv2.pyrDown(image)
            gaussian_pyr.append(image)

        laplacian_pyr = [gaussian_pyr[num_layers - 1]]
        for i in range(num_layers - 1, 0, -1):
            expanded = cv2.pyrUp(gaussian_pyr[i])
            laplacian = cv2.subtract(gaussian_pyr[i - 1], expanded)
            laplacian_pyr.append(laplacian)
        flattened_arrays = [arr.flatten() for arr in laplacian_pyr]
        x_feature = np.concatenate(flattened_arrays)
        X_features.append(x_feature)
    return X_features


def feature_extraction_fft(X_processed):
    X_features = []
    for x in X_processed:
        image = x
        f = np.fft.fft2(image)
        f_shift = np.fft.fftshift(f)
        magnitude_spectrum = 20 * np.log(np.abs(f_shift))
        magnitude_spectrum = magnitude_spectrum.reshape(-1)
        X_features.append(magnitude_spectrum)
    return X_features

def feature_extraction_HuMoments(X_processed):
    X_features = []
    for x in X_processed:
        image = x
        ret, binary = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)
        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        if (len(contours) == 0):
            hu_moments = np.zeros((7, 1))
        else:
            moments = cv2.moments(contours[0])
            hu_moments = cv2.HuMoments(moments)
        hu_moments = hu_moments.reshape(-1)
        X_features.append(hu_moments)
    return X_features

def split(X_features, y):
    X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=42)

    return X_train, X_test, y_train, y_test


models = [
    SVC(),  # Support Vector Classifier (SVC)
    # RandomForestClassifier(),  # Random Forest Classifier
    # KNeighborsClassifier(),  # k-Nearest Neighbors Classifier
    # DecisionTreeClassifier(),  # Decision Tree Classifier
    # GaussianNB(),  # Naive Bayes Classifier
    # MLPClassifier(max_iter=500)  # Artificial Neural Network (ANN)
]

X, y = dataloader()

precessed_methods = [
    processing_gray,
    processing_calcHist,
]

methods = [
    feature_extraction_Feature_Pyramid,
    feature_extraction_HOG,
    feature_extraction_local_binary,
    feature_extraction_fft,  # 耗时较长
    feature_extraction_HuMoments, # 只获取7个不变矩，直接应用于分类无明显实用性
]

for precessing in precessed_methods:
    X_processed = precessing(X)
    for method in methods:
        X_features = method(X_processed)
        X_train, X_test, y_train, y_test = split(X_features, y)
        for model in models:
            # Train the model
            model.fit(X_train, y_train)

            # Evaluate the model
            accuracy = model.score(X_test, y_test)
            print(
                f" precess_method: {precessing.__name__}, feature_extract_mathod: {method.__name__}, Model: {model.__class__.__name__}, Accuracy: {accuracy}")


